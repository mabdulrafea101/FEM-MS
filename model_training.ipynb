{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model for RC Beam Frequency Prediction\n",
    "## Part 2: Model Training and Validation\n",
    "\n",
    "**Objective:** Develop and validate machine learning models to predict natural frequencies of corroded RC beams.\n",
    "\n",
    "**Author:** Master's Research Project  \n",
    "**Date:** November 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in ./.venv12/lib/python3.12/site-packages (1.2.8)\n",
      "Requirement already satisfied: graphviz in ./.venv12/lib/python3.12/site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in ./.venv12/lib/python3.12/site-packages (from catboost) (3.10.7)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in ./.venv12/lib/python3.12/site-packages (from catboost) (2.3.5)\n",
      "Requirement already satisfied: pandas>=0.24 in ./.venv12/lib/python3.12/site-packages (from catboost) (2.3.3)\n",
      "Requirement already satisfied: scipy in ./.venv12/lib/python3.12/site-packages (from catboost) (1.16.3)\n",
      "Requirement already satisfied: plotly in ./.venv12/lib/python3.12/site-packages (from catboost) (6.5.0)\n",
      "Requirement already satisfied: six in ./.venv12/lib/python3.12/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv12/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv12/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv12/lib/python3.12/site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv12/lib/python3.12/site-packages (from matplotlib->catboost) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv12/lib/python3.12/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv12/lib/python3.12/site-packages (from matplotlib->catboost) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv12/lib/python3.12/site-packages (from matplotlib->catboost) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv12/lib/python3.12/site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv12/lib/python3.12/site-packages (from matplotlib->catboost) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv12/lib/python3.12/site-packages (from matplotlib->catboost) (3.2.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in ./.venv12/lib/python3.12/site-packages (from plotly->catboost) (2.12.0)\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv12/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv12/lib/python3.12/site-packages (from ipywidgets) (9.7.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv12/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator>=4.3.2 in ./.venv12/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in ./.venv12/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in ./.venv12/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in ./.venv12/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv12/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv12/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in ./.venv12/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in ./.venv12/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in ./.venv12/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv12/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv12/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv12/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv12/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv12/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "Using cached widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [ipywidgets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n",
      "1.2.8\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "import catboost\n",
    "print(catboost.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, r2_score\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minspection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m permutation_importance\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Logging System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "log_dir = Path('simulation/logs')\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_dir / 'ml_training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"ML MODEL TRAINING INITIALIZED\")\n",
    "logger.info(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = Path('simulation/data/beam_vibration_dataset.csv')\n",
    "logger.info(f\"Loading dataset from: {data_path}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    logger.info(f\"✓ Dataset loaded successfully: {df.shape[0]} samples, {df.shape[1]} features\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.any():\n",
    "    logger.warning(f\"Missing values detected:\\n{missing_values[missing_values > 0]}\")\n",
    "else:\n",
    "    logger.info(\"✓ No missing values detected\")\n",
    "\n",
    "# Data types\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for figures\n",
    "output_dir = Path('simulation/outputs/ml_figures')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Output directory created: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of input features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Distribution of Input Parameters', fontsize=16, fontweight='bold')\n",
    "\n",
    "features = ['Length', 'Width', 'Depth', 'Conc_Strength', 'Damage_Severity']\n",
    "for idx, feature in enumerate(features):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.hist(df[feature], bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel(feature, fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_title(f'{feature} Distribution', fontsize=13)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Damage type distribution\n",
    "ax = axes[1, 2]\n",
    "df['Damage_Type'].value_counts().plot(kind='bar', ax=ax, edgecolor='black')\n",
    "ax.set_xlabel('Damage Type', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Damage Type Distribution', fontsize=13)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'parameter_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "logger.info(\"✓ Parameter distribution plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix of Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "logger.info(\"✓ Correlation matrix saved\")\n",
    "\n",
    "# Log key correlations with target\n",
    "print(\"\\nCorrelations with Mode 1 Frequency:\")\n",
    "print(correlation_matrix['Freq_Mode_1'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damage severity vs frequency\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Mode 1\n",
    "axes[0].scatter(df['Damage_Severity'], df['Freq_Mode_1'], alpha=0.5, s=20)\n",
    "axes[0].set_xlabel('Damage Severity (%)', fontsize=12)\n",
    "axes[0].set_ylabel('Mode 1 Frequency (Hz)', fontsize=12)\n",
    "axes[0].set_title('Damage Severity vs Mode 1 Frequency', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Mode 2\n",
    "axes[1].scatter(df['Damage_Severity'], df['Freq_Mode_2'], alpha=0.5, s=20, color='orange')\n",
    "axes[1].set_xlabel('Damage Severity (%)', fontsize=12)\n",
    "axes[1].set_ylabel('Mode 2 Frequency (Hz)', fontsize=12)\n",
    "axes[1].set_title('Damage Severity vs Mode 2 Frequency', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'damage_vs_frequency.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "logger.info(\"✓ Damage vs frequency plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "logger.info(\"Preparing features and target variables\")\n",
    "\n",
    "# Define features and targets\n",
    "feature_cols = ['Length', 'Width', 'Depth', 'Conc_Strength', 'Damage_Severity']\n",
    "target_col = 'Freq_Mode_1'\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "logger.info(f\"Features shape: {X.shape}\")\n",
    "logger.info(f\"Target shape: {y.shape}\")\n",
    "logger.info(f\"Feature columns: {feature_cols}\")\n",
    "\n",
    "print(f\"\\nFeatures (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split with stratification\n",
    "logger.info(\"Splitting data into train and test sets (80/20)\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, shuffle=True\n",
    ")\n",
    "\n",
    "logger.info(f\"Train set: {X_train.shape[0]} samples\")\n",
    "logger.info(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\nTraining samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "logger.info(\"Applying StandardScaler to features\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logger.info(\"✓ Feature scaling completed\")\n",
    "print(\"\\n✓ Data preprocessing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store models and results\n",
    "models = {}\n",
    "results = []\n",
    "\n",
    "def evaluate_model(name, model, X_tr, y_tr, X_te, y_te, use_scaling=True):\n",
    "    \"\"\"Train and evaluate a model with comprehensive metrics.\"\"\"\n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"Training {name}\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    \n",
    "    # Select scaled or unscaled data\n",
    "    X_train_data = X_tr if use_scaling else X_train\n",
    "    X_test_data = X_te if use_scaling else X_test\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_data, y_tr)\n",
    "    logger.info(f\"✓ {name} training completed\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_data)\n",
    "    y_test_pred = model.predict(X_test_data)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mae = mean_absolute_error(y_tr, y_train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_tr, y_train_pred))\n",
    "    train_r2 = r2_score(y_tr, y_train_pred)\n",
    "    \n",
    "    test_mae = mean_absolute_error(y_te, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_te, y_test_pred))\n",
    "    test_r2 = r2_score(y_te, y_test_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_data, y_tr, cv=5, scoring='r2')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Log results\n",
    "    logger.info(f\"Train MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}\")\n",
    "    logger.info(f\"Test MAE: {test_mae:.4f}, RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
    "    logger.info(f\"CV R² Score: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train_MAE': train_mae,\n",
    "        'Train_RMSE': train_rmse,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_MAE': test_mae,\n",
    "        'Test_RMSE': test_rmse,\n",
    "        'Test_R2': test_r2,\n",
    "        'CV_R2_Mean': cv_mean,\n",
    "        'CV_R2_Std': cv_std\n",
    "    })\n",
    "    \n",
    "    return model, y_test_pred\n",
    "\n",
    "print(\"Model evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Linear Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_trained, lr_predictions = evaluate_model(\n",
    "    'Linear Regression', lr_model, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "models['Linear Regression'] = (lr_trained, lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf_trained, rf_predictions = evaluate_model(\n",
    "    'Random Forest', rf_model, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "models['Random Forest'] = (rf_trained, rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_trained, xgb_predictions = evaluate_model(\n",
    "    'XGBoost', xgb_model, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "models['XGBoost'] = (xgb_trained, xgb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 CatBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_model = CatBoostRegressor(\n",
    "    iterations=200,\n",
    "    depth=8,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=False\n",
    ")\n",
    "cb_trained, cb_predictions = evaluate_model(\n",
    "    'CatBoost', cb_model, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "models['CatBoost'] = (cb_trained, cb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model = SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1)\n",
    "svr_trained, svr_predictions = evaluate_model(\n",
    "    'SVR', svr_model, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "models['SVR'] = (svr_trained, svr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(output_dir / 'model_comparison.csv', index=False)\n",
    "logger.info(\"✓ Results saved to CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison bar plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "metrics = ['Test_MAE', 'Test_RMSE', 'Test_R2']\n",
    "titles = ['Mean Absolute Error (Test)', 'Root Mean Squared Error (Test)', 'R² Score (Test)']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx]\n",
    "    results_df.plot(x='Model', y=metric, kind='bar', ax=ax, legend=False, color='steelblue', edgecolor='black')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(metric.replace('_', ' '), fontsize=12)\n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "logger.info(\"✓ Model comparison plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Actual for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, (model, predictions)) in enumerate(models.items()):\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(y_test, predictions, alpha=0.5, s=30)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test.min(), predictions.min())\n",
    "    max_val = max(y_test.max(), predictions.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    ax.set_title(f'{model_name}\\nR² = {r2:.4f}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Actual Frequency (Hz)', fontsize=11)\n",
    "    ax.set_ylabel('Predicted Frequency (Hz)', fontsize=11)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Hide last subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'prediction_vs_actual.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "logger.info(\"✓ Prediction vs actual plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots for top 3 models\n",
    "top_models = results_df.nlargest(3, 'Test_R2')['Model'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "for idx, model_name in enumerate(top_models):\n",
    "    model, predictions = models[model_name]\n",
    "    residuals = y_test - predictions\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.scatter(predictions, residuals, alpha=0.5, s=30)\n",
    "    ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    ax.set_title(f'{model_name}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Frequency (Hz)', fontsize=11)\n",
    "    ax.set_ylabel('Residuals (Hz)', fontsize=11)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'residual_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "logger.info(\"✓ Residual plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "tree_models = ['Random Forest', 'XGBoost', 'CatBoost']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "for idx, model_name in enumerate(tree_models):\n",
    "    model, _ = models[model_name]\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        importances = model.get_feature_importance()\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    y_pos = np.arange(len(feature_cols))\n",
    "    ax.barh(y_pos, importances, edgecolor='black')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(feature_cols)\n",
    "    ax.set_xlabel('Importance', fontsize=11)\n",
    "    ax.set_title(f'{model_name} Feature Importance', fontsize=13, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "logger.info(\"✓ Feature importance plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP for best model\n",
    "best_model_name = results_df.loc[results_df['Test_R2'].idxmax(), 'Model']\n",
    "best_model, _ = models[best_model_name]\n",
    "\n",
    "logger.info(f\"\\nPerforming SHAP analysis for best model: {best_model_name}\")\n",
    "\n",
    "# Create SHAP explainer\n",
    "if best_model_name in ['Random Forest', 'XGBoost', 'CatBoost']:\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test_scaled)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test_scaled, feature_names=feature_cols, show=False)\n",
    "    plt.title(f'SHAP Summary Plot - {best_model_name}', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    logger.info(\"✓ SHAP summary plot saved\")\n",
    "else:\n",
    "    logger.info(\"SHAP analysis not available for this model type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model and scaler\n",
    "model_dir = Path('simulation/models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(best_model, model_dir / f'best_model_{best_model_name.replace(\" \", \"_\")}.pkl')\n",
    "joblib.dump(scaler, model_dir / 'scaler.pkl')\n",
    "\n",
    "logger.info(f\"✓ Best model ({best_model_name}) saved to {model_dir}\")\n",
    "logger.info(f\"✓ Scaler saved to {model_dir}\")\n",
    "\n",
    "print(f\"\\n✓ Best model: {best_model_name}\")\n",
    "print(f\"✓ Saved to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Prediction Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_frequency(length, width, depth, conc_strength, damage_severity):\n",
    "    \"\"\"\n",
    "    Predict natural frequency for given beam parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    length : float - Beam length (m)\n",
    "    width : float - Beam width (m)\n",
    "    depth : float - Beam depth (m)\n",
    "    conc_strength : float - Concrete strength (MPa)\n",
    "    damage_severity : float - Damage severity (0-100%)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float - Predicted Mode 1 frequency (Hz)\n",
    "    \"\"\"\n",
    "    # Create input array\n",
    "    input_data = np.array([[length, width, depth, conc_strength, damage_severity]])\n",
    "    \n",
    "    # Scale input\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = best_model.predict(input_scaled)[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test prediction function\n",
    "test_length = 4.0\n",
    "test_width = 0.3\n",
    "test_depth = 0.5\n",
    "test_strength = 35\n",
    "test_damage = 10\n",
    "\n",
    "pred_freq = predict_frequency(test_length, test_width, test_depth, test_strength, test_damage)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTION TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Input Parameters:\")\n",
    "print(f\"  Length: {test_length} m\")\n",
    "print(f\"  Width: {test_width} m\")\n",
    "print(f\"  Depth: {test_depth} m\")\n",
    "print(f\"  Concrete Strength: {test_strength} MPa\")\n",
    "print(f\"  Damage Severity: {test_damage}%\")\n",
    "print(f\"\\nPredicted Mode 1 Frequency: {pred_freq:.2f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset Size: {len(df)} samples\")\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Testing Samples: {len(X_test)}\")\n",
    "print(f\"\\nFeatures: {', '.join(feature_cols)}\")\n",
    "print(f\"\\nModels Trained: {len(models)}\")\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "best_metrics = results_df[results_df['Model'] == best_model_name].iloc[0]\n",
    "print(f\"  Test R²: {best_metrics['Test_R2']:.4f}\")\n",
    "print(f\"  Test MAE: {best_metrics['Test_MAE']:.4f} Hz\")\n",
    "print(f\"  Test RMSE: {best_metrics['Test_RMSE']:.4f} Hz\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"ML TRAINING COMPLETED SUCCESSFULLY\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Best Model: {best_model_name} (R² = {best_metrics['Test_R2']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
